{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Control_Pendulum",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmejiaho/TAM-2025-2/blob/main/Control_Pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:27:39.934137Z",
          "iopub.execute_input": "2025-11-20T21:27:39.934727Z",
          "iopub.status.idle": "2025-11-20T21:27:41.679732Z",
          "shell.execute_reply.started": "2025-11-20T21:27:39.934701Z",
          "shell.execute_reply": "2025-11-20T21:27:41.67892Z"
        },
        "id": "s0G9yV67lCh2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install -q gym[classic_control]\n",
        "!pip install -q tf-agents[reverb]\n",
        "!pip install -q imageio\n",
        "!pip install -q pyvirtualdisplay"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:49:28.401857Z",
          "iopub.execute_input": "2025-11-20T21:49:28.402684Z",
          "iopub.status.idle": "2025-11-20T21:49:46.730316Z",
          "shell.execute_reply.started": "2025-11-20T21:49:28.402657Z",
          "shell.execute_reply": "2025-11-20T21:49:46.729331Z"
        },
        "id": "zRmbUmhslCh6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# Configuración de pantalla virtual\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# --- CAMBIO: Importamos SAC y redes de distribución ---\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import actor_distribution_network # Necesario para SAC\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:58:14.792369Z",
          "iopub.execute_input": "2025-11-20T21:58:14.793277Z",
          "iopub.status.idle": "2025-11-20T21:58:14.883105Z",
          "shell.execute_reply.started": "2025-11-20T21:58:14.793227Z",
          "shell.execute_reply": "2025-11-20T21:58:14.882442Z"
        },
        "id": "J_iIRv7llCh7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuración de los Hiperparámetros**\n",
        "\n",
        "En este proceso, se definen las \"reglas del juego\" y la \"personalidad\" del agente antes de que empiece a estudiar.\n",
        "\n",
        "Se define cuánto tiempo va a entrenar (iteraciones).\n",
        "\n",
        "Se decide cuánta memoria va a tener para recordar sus errores y aciertos.\n",
        "\n",
        "Se ajusta la velocidad a la que queremos que aprenda (ni muy rápido para no despistarse, ni muy lento para no tardar años).\n",
        "\n",
        "Se establece cada cuánto tiempo vamos a hacerle un examen para ver si está mejorando."
      ],
      "metadata": {
        "id": "eQtqZgDElCh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 50000\n",
        "\n",
        "initial_collect_steps = 10000 # Recolectar mas experiencia inicial\n",
        "collect_steps_per_iteration = 1\n",
        "replay_buffer_max_length = 1000000\n",
        "\n",
        "# Batch size más grande para mejor estabilidad en continuo\n",
        "batch_size = 256\n",
        "\n",
        "# Tasas de aprendizaje estándar para SAC\n",
        "learning_rate_actor = 3e-4\n",
        "learning_rate_critic = 3e-4\n",
        "learning_rate_alpha = 3e-4\n",
        "\n",
        "log_interval = 1000\n",
        "num_eval_episodes = 10\n",
        "eval_interval = 2000"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T21:58:53.647882Z",
          "iopub.execute_input": "2025-11-20T21:58:53.648166Z",
          "iopub.status.idle": "2025-11-20T21:58:53.652685Z",
          "shell.execute_reply.started": "2025-11-20T21:58:53.648145Z",
          "shell.execute_reply": "2025-11-20T21:58:53.652067Z"
        },
        "id": "60_pVGAOlCh_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparación del Entorno (El Mundo Virtual)**\n",
        "\n",
        "En este proceso, se carga el simulador donde vivirá nuestro agente.\n",
        "\n",
        "Se elige el \"juego\" o problema que queremos resolver (en este caso, el Péndulo).\n",
        "\n",
        "Se crean dos copias del mundo: una para que el agente practique (entrenamiento) y otra para examinarlo (evaluación).\n",
        "\n",
        "Se traduce este mundo a un formato que las herramientas de Inteligencia Artificial (TensorFlow) puedan entender.\n",
        "\n",
        "Se revisa qué información nos da el mundo (observaciones) y qué movimientos permite hacer (acciones)."
      ],
      "metadata": {
        "id": "DArojm1ulCiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta línea define el nombre del entorno que vamos a cargar.\n",
        "env_name = 'Pendulum-v1'\n",
        "\n",
        "# Esta línea carga el entorno para que el agente practique y aprenda.\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "\n",
        "# Esta línea carga una copia separada del entorno solo para examinar al agente.\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "# Esta línea convierte el entorno de práctica a un formato compatible con TensorFlow.\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "\n",
        "# Esta línea convierte el entorno de examen a un formato compatible con TensorFlow.\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# Esta línea muestra qué datos recibe el agente (posición, velocidad).\n",
        "print('Observation Spec:', train_env.observation_spec())\n",
        "\n",
        "# Esta línea muestra qué acciones puede tomar el agente (fuerza del torque).\n",
        "print('Action Spec:', train_env.action_spec())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:01:21.904239Z",
          "iopub.execute_input": "2025-11-20T22:01:21.904598Z",
          "iopub.status.idle": "2025-11-20T22:01:21.9206Z",
          "shell.execute_reply.started": "2025-11-20T22:01:21.904576Z",
          "shell.execute_reply": "2025-11-20T22:01:21.919723Z"
        },
        "id": "RSuCw8H2lCiB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Construcción del Cerebro (Redes Neuronales)**\n",
        "\n",
        "En este proceso, se crean las estructuras mentales del agente.\n",
        "\n",
        "Se define el tamaño del cerebro, aumentando la cantidad de neuronas para que tenga mejor motricidad fina.\n",
        "\n",
        "Se construye la red \"Actor\", que es la responsable de decidir qué acción tomar en cada momento.\n",
        "\n",
        "Se construye la red \"Crítico\", que se encarga de juzgar si las acciones tomadas fueron buenas o malas."
      ],
      "metadata": {
        "id": "ut5e6054lCiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc_layer_params = (256, 256)\n",
        "\n",
        "# 1. Red Actor (Distribución): Predice media y desviación estándar de la acción\n",
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params,\n",
        "    activation_fn=tf.keras.activations.relu)\n",
        "\n",
        "# 2. Red Critic: Estima el valor Q (SAC usa esto igual que DDPG)\n",
        "critic_net = critic_network.CriticNetwork(\n",
        "    (train_env.observation_spec(), train_env.action_spec()),\n",
        "    observation_fc_layer_params=None,\n",
        "    action_fc_layer_params=fc_layer_params,\n",
        "    joint_fc_layer_params=fc_layer_params,\n",
        "    activation_fn=tf.keras.activations.relu)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:01:27.109533Z",
          "iopub.execute_input": "2025-11-20T22:01:27.110272Z",
          "iopub.status.idle": "2025-11-20T22:01:27.126867Z",
          "shell.execute_reply.started": "2025-11-20T22:01:27.110227Z",
          "shell.execute_reply": "2025-11-20T22:01:27.12624Z"
        },
        "id": "VaQ7Rc1ClCiC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensamblaje del Agente (El Jugador)**\n",
        "\n",
        "En este proceso, se une todo para crear al agente completo.\n",
        "\n",
        "Se configura el algoritmo SAC, que es el \"cerebro\" que usará las redes anteriores para aprender.\n",
        "\n",
        "Se le asignan los optimizadores, que son las herramientas matemáticas para ajustar sus neuronas y mejorar.\n",
        "\n",
        "Se preparan las políticas: una para recolectar datos explorando y otra para actuar cuando ya sabe qué hacer."
      ],
      "metadata": {
        "id": "629-lZxalCiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se normalizan las observaciones para ayudar a la red\n",
        "tf_agent = sac_agent.SacAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    actor_network=actor_net,\n",
        "    critic_network=critic_net,\n",
        "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate_actor),\n",
        "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate_critic),\n",
        "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate_alpha),\n",
        "    target_update_tau=0.005,\n",
        "    target_update_period=1,\n",
        "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
        "    gamma=0.99,\n",
        "    reward_scale_factor=1.0, # SAC maneja bien la escala natural\n",
        "    train_step_counter=tf.Variable(0))\n",
        "\n",
        "# Esta línea termina de configurar el agente para que esté listo.\n",
        "tf_agent.initialize()\n",
        "\n",
        "# Esta línea guarda la política principal que usaremos para evaluar al agente.\n",
        "eval_policy = tf_agent.policy\n",
        "\n",
        "# Esta línea guarda la política de recolección que usa el agente para explorar mientras aprende.\n",
        "collect_policy = tf_agent.collect_policy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:01:32.633884Z",
          "iopub.execute_input": "2025-11-20T22:01:32.634183Z",
          "iopub.status.idle": "2025-11-20T22:01:32.825661Z",
          "shell.execute_reply.started": "2025-11-20T22:01:32.634163Z",
          "shell.execute_reply": "2025-11-20T22:01:32.824993Z"
        },
        "id": "qGVMLQuBlCiD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuración de la Memoria**\n",
        "\n",
        "En este proceso, se prepara el sistema de almacenamiento de experiencias.\n",
        "\n",
        "Se define una función auxiliar para calcular el promedio de notas del agente.\n",
        "\n",
        "Se crea el \"Buffer de Repetición\", que es un almacén gigante donde se guardan todas las jugadas pasadas para repasarlas después."
      ],
      "metadata": {
        "id": "kAJskBTzlCiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular métricas\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "# Crear el Replay Buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:01:54.526121Z",
          "iopub.execute_input": "2025-11-20T22:01:54.526774Z",
          "iopub.status.idle": "2025-11-20T22:01:54.56152Z",
          "shell.execute_reply.started": "2025-11-20T22:01:54.52675Z",
          "shell.execute_reply": "2025-11-20T22:01:54.560911Z"
        },
        "id": "ot3JNwrMlCiD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recolección Inicial de Datos**\n",
        "\n",
        "En este proceso, se llena la memoria del agente antes de empezar el entrenamiento serio.\n",
        "\n",
        "Se utiliza un \"conductor\" (driver) que mueve al agente por el mundo para acumular experiencias.\n",
        "\n",
        "Se ejecuta la recolección inicial para tener datos base.\n",
        "\n",
        "Se organiza la memoria en un formato eficiente para poder leerla rápidamente durante el entrenamiento."
      ],
      "metadata": {
        "id": "1aISdZVPlCiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Driver para recolectar experiencias iniciales\n",
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_env,\n",
        "    collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=initial_collect_steps)\n",
        "\n",
        "collect_driver.run()\n",
        "\n",
        "# Crear dataset para entrenamiento\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:02:11.451965Z",
          "iopub.execute_input": "2025-11-20T22:02:11.452234Z",
          "iopub.status.idle": "2025-11-20T22:05:51.553676Z",
          "shell.execute_reply.started": "2025-11-20T22:02:11.452213Z",
          "shell.execute_reply": "2025-11-20T22:05:51.552867Z"
        },
        "id": "flbVytI8lCiE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bucle de Entrenamiento Principal**\n",
        "\n",
        "En este proceso, ocurre el aprendizaje real del agente.\n",
        "\n",
        "Se evalúa al agente antes de empezar para saber su nivel inicial.\n",
        "\n",
        "Se entra en un ciclo repetitivo donde el agente alterna entre explorar un poco más y estudiar lo que ya vivió.\n",
        "\n",
        "Se actualizan las redes neuronales usando los datos de la memoria.\n",
        "\n",
        "Se imprime el progreso y se evalúa periódicamente para ver la mejora."
      ],
      "metadata": {
        "id": "KF9kAEsqlCiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_agent.train = common.function(tf_agent.train)\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluamos antes de empezar\n",
        "avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "print(f'Retorno inicial (sin entrenar): {avg_return}')\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # 1. Recolectar\n",
        "  collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    train_env,\n",
        "    collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=collect_steps_per_iteration)\n",
        "  collect_driver.run()\n",
        "\n",
        "  # 2. Muestrear\n",
        "  experience, _ = next(iterator)\n",
        "\n",
        "  # 3. Entrenar\n",
        "  train_loss = tf_agent.train(experience).loss\n",
        "\n",
        "  step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print(f'step = {step}: loss = {train_loss}')\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
        "    print(f'step = {step}: Average Return = {avg_return}')\n",
        "    returns.append(avg_return)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:06:04.638356Z",
          "iopub.execute_input": "2025-11-20T22:06:04.638987Z",
          "iopub.status.idle": "2025-11-20T22:54:31.646221Z",
          "shell.execute_reply.started": "2025-11-20T22:06:04.63896Z",
          "shell.execute_reply": "2025-11-20T22:54:31.645359Z"
        },
        "id": "igA5HWLHlCiF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualización de Resultados**\n",
        "\n",
        "En este proceso, se generan gráficos para entender visualmente la evolución del agente.\n",
        "\n",
        "Se crea un gráfico que muestra cómo ha cambiado la puntuación del agente a lo largo del tiempo.\n",
        "\n",
        "Se interpreta la curva: si sube, significa que el agente está aprendiendo a mantener el péndulo arriba."
      ],
      "metadata": {
        "id": "O4v-PPEmlCiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.title(\"Entrenamiento SAC en Pendulum-v1\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:57:03.731272Z",
          "iopub.execute_input": "2025-11-20T22:57:03.731581Z",
          "iopub.status.idle": "2025-11-20T22:57:03.893362Z",
          "shell.execute_reply.started": "2025-11-20T22:57:03.731558Z",
          "shell.execute_reply": "2025-11-20T22:57:03.892689Z"
        },
        "id": "8hrYZmLElCiF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "El inicio : La línea empieza muy abajo (valores muy negativos, como -1500). Esto significa que al principio el agente no sabía nada y el péndulo se caía o giraba sin control.\n",
        "\n",
        "La subida : Vemos que la línea sube rápidamente. Este es el momento donde el agente descubre la técnica para levantar el péndulo y empieza a ganar mejores puntos."
      ],
      "metadata": {
        "id": "l9wouzwnlCiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generación del Video Final**\n",
        "\n",
        "En este proceso, se graba al agente en acción para verificar visualmente su comportamiento.\n",
        "\n",
        "Se define una función para grabar varios intentos del agente y guardarlos en un archivo de video.\n",
        "\n",
        "Se define otra función para incrustar ese video dentro de este cuaderno y poder verlo aquí mismo.\n",
        "\n",
        "Se ejecuta la grabación y se muestra el resultado final."
      ],
      "metadata": {
        "id": "Lll1qVo0lCiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para crear el video (igual que antes)\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      # Renderiza el primer frame\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        # Renderiza frames siguientes\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return filename\n",
        "\n",
        "# --- NUEVA FUNCIÓN PARA VISUALIZAR EL VIDEO ---\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Incrusta un archivo MP4 en el cuaderno usando HTML y Base64.\"\"\"\n",
        "  video = open(filename, 'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 1. Generar el archivo de video\n",
        "video_filename = create_policy_eval_video(eval_policy, \"trained-agent\")\n",
        "\n",
        "# 2. Mostrar el video en el cuaderno\n",
        "print(f\"Video guardado como: {video_filename}\")\n",
        "embed_mp4(video_filename)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-20T22:57:11.217223Z",
          "iopub.execute_input": "2025-11-20T22:57:11.21756Z",
          "iopub.status.idle": "2025-11-20T22:57:35.248902Z",
          "shell.execute_reply.started": "2025-11-20T22:57:11.217538Z",
          "shell.execute_reply": "2025-11-20T22:57:35.248178Z"
        },
        "id": "9XkGWuLllCiG"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}